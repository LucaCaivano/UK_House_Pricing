{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main_nooutput.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Machine learning exercise- Pricing UK Houses"
      ],
      "metadata": {
        "id": "eQsX_woOrbam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries."
      ],
      "metadata": {
        "id": "ZZ4ZIqWhvaMB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pNAMHdQ2eK5M"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from sklearn import metrics\n",
        "random.seed(27)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data were shuffled on my local system through an external library written in C++ (https://github.com/alexandres/terashuf): this was done in order to avoid the indroduction of bias in the training of the Neural Newtork. Indeed data are more or less ordered by date in the original csv. Given the big dimension of the .csv file, data are read by chunks in order to make them feasible to the RAM : these chunks work as additional batches in the training of the Neural Network."
      ],
      "metadata": {
        "id": "MmORVXK5bAa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path='/content/drive/MyDrive/Homework_Amazon/pp-complete_shuffled.csv'\n",
        "chunksize = 10 ** 6\n",
        "index=[1, 2, 4, 6, 11]\n",
        "lst=['Id', 'Price', 'Date',  \n",
        "    'Postcode', 'Property_type', 'Old_new', 'Duration', 'PAON', 'SAON',\n",
        "    'Street', 'Locality', 'Town', 'District', 'County', 'PPD', 'Record_status', 'unknown1', 'unknokn2'] \n",
        "colnames=['price', 'data', 'property_type', 'lease_duration', 'location']\n",
        "pd.read_csv(data_path, chunksize=chunksize, names=lst, header=None)"
      ],
      "metadata": {
        "id": "hi5P-dV4j8zT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66ae3144-b29f-4e4d-dba2-83b6b4a1e113"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.parsers.readers.TextFileReader at 0x7f7e05a44fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing and compiling the Neural Network: 3 hidden layers on 10 neurons each, all with RELU activation functions. Moreover an Adam optimizer is adopted using mean squared error as loss function."
      ],
      "metadata": {
        "id": "iiqw-u_n0pej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(9,)),\n",
        "    keras.layers.Dense(10, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='relu'),\n",
        "\n",
        "\n",
        "    keras.layers.Dense(1),\n",
        "])"
      ],
      "metadata": {
        "id": "lvnYUPzIOyEA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = keras.optimizers.Adam(learning_rate=0.1),\n",
        "              loss = keras.losses.MeanSquaredError(),\n",
        "              metrics=['MeanSquaredError'])"
      ],
      "metadata": {
        "id": "4INQKJZSPwQ2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing on categorical variables: one hot encoding and extracting bool variable for 'Town' (London or not). Splitting on dataset in train and test based on the date (data after 01-01-2019 is added to the testset). The train is again splitted in training and validation with 67%-33% ratio. The neural network is then trained in every step of the loop with a new chunk."
      ],
      "metadata": {
        "id": "6PkBGRZki-YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "i=0\n",
        "for chunk in pd.read_csv(data_path, chunksize=chunksize, names=lst, header=None):\n",
        "  #training preprocessing\n",
        "  chunk[\"Date\"]= pd.to_datetime(chunk[\"Date\"])\n",
        "  batch=chunk[chunk.Date<pd.datetime(2019, 1, 1)]\n",
        "  batch['Property_type'] = batch['Property_type'].astype('category')\n",
        "  property_type_batch =pd.get_dummies(batch.Property_type, prefix='Property')\n",
        "  batch['Duration'] = batch['Duration'].astype('category')\n",
        "  duration_batch=pd.get_dummies(batch.Duration, prefix='Duration')\n",
        "  london_bool_batch=np.array(batch.Town=='LONDON')\n",
        "  london_dummy_batch=london_bool_batch.astype(int)\n",
        "  london_dummy_batch=london_dummy_batch.reshape(london_dummy_batch.shape[0], 1)\n",
        "  y_batch=batch['Price'].to_numpy()\n",
        "  y_batch=y_batch.reshape(y_batch.shape[0], 1)\n",
        "  y_batch=y_batch.astype('float')\n",
        "  X_batch=property_type_batch.to_numpy()\n",
        "  X_batch=np.append(X_batch, duration_batch, axis=1)\n",
        "  if(X_batch.shape[1]!=8): #adding extra column to durations dummy (if no duration_U values are present)\n",
        "    X_batch=np.append(X_batch, np.zeros((X_batch.shape[0], 1)), axis=1)\n",
        "    print('No duration_U in this chunk')\n",
        "  X_batch=np.append(X_batch, london_dummy_batch, axis=1)\n",
        "  X_batch.shape, y_batch.shape\n",
        "  X_train, X_validation, y_train, y_validation = train_test_split(X_batch, y_batch, test_size=0.33, random_state=101)\n",
        "  \n",
        "  #test preprocessing\n",
        "  test_chunk=chunk[chunk.Date>=pd.datetime(2019, 1, 1)]\n",
        "  test_chunk['Property_type'] = test_chunk['Property_type'].astype('category')\n",
        "  property_type_test_chunk =pd.get_dummies(test_chunk.Property_type, prefix='Property')\n",
        "  duration_test_chunk=pd.get_dummies(test_chunk.Duration, prefix='Duration')\n",
        "  london_bool_test_chunk=np.array(test_chunk.Town=='London')\n",
        "  london_dummy_test_chunk=london_bool_test_chunk.astype(int)\n",
        "  london_dummy_test_chunk=london_dummy_test_chunk.reshape(london_dummy_test_chunk.shape[0], 1)\n",
        "  y_test_chunk=test_chunk['Price'].to_numpy()\n",
        "  y_test_chunk=y_test_chunk.reshape(y_test_chunk.shape[0], 1)\n",
        "  y_test_chunk=y_test_chunk.astype('float')\n",
        "  X_test_chunk=property_type_test_chunk.to_numpy()\n",
        "  X_test_chunk=np.append(X_test_chunk, duration_test_chunk, axis=1)\n",
        "  X_test_chunk=np.append(X_test_chunk, np.zeros((X_test_chunk.shape[0], 1)), axis=1) #adding extra column to durations dummy (no U values are preseny since 2019)\n",
        "  X_test_chunk=np.append(X_test_chunk, london_dummy_test_chunk, axis=1)\n",
        "  if(i==0):  #first chunk: initialization of X_test and y_test\n",
        "    X_test=X_test_chunk\n",
        "    y_test=y_test_chunk\n",
        "  else:\n",
        "    X_test=np.append(X_test, X_test_chunk, axis=0)\n",
        "    y_test=np.append(y_test, y_test_chunk, axis=0)\n",
        "  #training of the model\n",
        "  history=model.fit(X_train, y_train, epochs = 2, validation_data = (X_validation, y_validation), batch_size=100)\n",
        "  print(i)\n",
        "  i=i+1\n"
      ],
      "metadata": {
        "id": "0nTm0EpdnkWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing on the dataset"
      ],
      "metadata": {
        "id": "OFngA_7V0uMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted = model.predict(X_test)\n",
        "y_predicted=y_predicted.flatten()\n",
        "# evaluate predictions\n",
        "print('MAE:', metrics.mean_absolute_error(y_test, y_predicted))  \n",
        "print('MSE:', metrics.mean_squared_error(y_test, y_predicted))  \n",
        "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_predicted)))"
      ],
      "metadata": {
        "id": "t9P39riqrLHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c827f67d-85c7-4218-edcb-d5b776fac86c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 241219.7352421483\n",
            "MSE: 3411288110175.523\n",
            "RMSE: 1846967.2737153529\n"
          ]
        }
      ]
    }
  ]
}